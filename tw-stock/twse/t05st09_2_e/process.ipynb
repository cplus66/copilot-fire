{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46cd88-9646-4bd1-9a0d-79ab49366b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the HTML file using Big5 encoding\n",
    "with open(\"t05st09_2_e_2024.html\", \"r\", encoding=\"big5\", errors=\"ignore\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# Step 2: Extract all tables using pandas\n",
    "tables = pd.read_html(str(soup))\n",
    "\n",
    "# Step 3: Select the third table (index 2)\n",
    "df = tables[2] if len(tables) > 2 else pd.DataFrame()\n",
    "\n",
    "# Step 4: Drop rows where all elements are NaN\n",
    "df.dropna(how='all', inplace=True)\n",
    "\n",
    "# Step 5: Remove duplicate header rows\n",
    "df = df[df[df.columns[0]] != df.columns[0]]\n",
    "\n",
    "# Step 6: Replace empty or NaN values in the first column with \"Company\"\n",
    "df.iloc[:, 0] = df.iloc[:, 0].fillna(\"Company\")\n",
    "df.iloc[:, 0] = df.iloc[:, 0].replace(\"\", \"Company\")\n",
    "\n",
    "# Step 7: Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 8: Save the cleaned DataFrame to CSV\n",
    "df.to_csv(\"updated_taiwan_stock_dividends_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1075a-3ae2-464c-ae70-df7bfaaff769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828c958-ade2-4bd0-b4a3-ed918950b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the HTML file using Big5 encoding\n",
    "with open(\"t05st09_2_e_2024.html\", \"r\", encoding=\"big5\", errors=\"ignore\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# Step 2: Extract all tables using pandas\n",
    "tables = pd.read_html(str(soup))\n",
    "\n",
    "# Step 3: Select the third table (index 2)\n",
    "if len(tables) > 2:\n",
    "    df = tables[2]\n",
    "\n",
    "    # Step 4: Drop rows where all elements are NaN\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Step 5: Remove duplicate header rows\n",
    "    df = df[df[df.columns[0]] != df.columns[0]]\n",
    "\n",
    "    # Step 6: Replace empty or NaN values in the first column with \"Company\"\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].fillna(\"Company\")\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].replace(\"\", \"Company\")\n",
    "\n",
    "    # Step 7: Remove the first row and set the second row as header\n",
    "    df.columns = df.iloc[1]  # Set second row as header\n",
    "    df = df[2:]              # Remove the first two rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Step 8: Save the final cleaned DataFrame to CSV\n",
    "    df.to_csv(\"final_taiwan_stock_dividends_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab85ce4-be8b-423c-a878-4310be8a295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Step 1: Read the HTML file using Big5 encoding\n",
    "with open(\"t05st09_2_e_2024.html\", \"r\", encoding=\"big5\", errors=\"ignore\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# Step 2: Wrap HTML content in StringIO to avoid FutureWarning\n",
    "html_str = str(soup)\n",
    "html_io = StringIO(html_str)\n",
    "\n",
    "# Step 3: Extract all tables using pandas\n",
    "tables = pd.read_html(html_io)\n",
    "\n",
    "# Step 4: Select the third table (index 2)\n",
    "if len(tables) > 2:\n",
    "    df = tables[2]\n",
    "\n",
    "    # Drop rows where all elements are NaN\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Remove duplicate header rows\n",
    "    df = df[df[df.columns[0]] != df.columns[0]]\n",
    "\n",
    "    # Replace empty or NaN values in the first column with \"Company\"\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].fillna(\"Company\")\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].replace(\"\", \"Company\")\n",
    "\n",
    "    # Remove the first row and set the second row as header\n",
    "    df.columns = df.iloc[1]  # Set second row as header\n",
    "    df = df[2:]              # Remove the first two rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the final cleaned DataFrame to CSV\n",
    "    df.to_csv(\"final_taiwan_stock_dividends_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b0371-9a31-43fc-a76a-fbed7b91c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071c1cc-2a25-4349-8edd-ab06a86ddf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "# Extract year from filename\n",
    "filename = \"t05st09_2_e_2024.html\"\n",
    "year_match = re.search(r\"t05st09_2_e_(\\\\d{4})\", filename)\n",
    "year = year_match.group(1) if year_match else \"Unknown\"\n",
    "\n",
    "# Read the HTML file using Big5 encoding\n",
    "with open(filename, \"r\", encoding=\"big5\", errors=\"ignore\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# Wrap HTML content in StringIO to avoid FutureWarning\n",
    "html_io = StringIO(str(soup))\n",
    "\n",
    "# Extract all tables using pandas\n",
    "tables = pd.read_html(html_io)\n",
    "\n",
    "# Extract and clean the third table\n",
    "if len(tables) > 2:\n",
    "    df = tables[2]\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df = df[df[df.columns[0]] != df.columns[0]]\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].fillna(\"Company\").replace(\"\", \"Company\")\n",
    "    df.columns = df.iloc[1]  # Set second row as header\n",
    "    df = df[2:]              # Remove the first two rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add the Year column\n",
    "    df.insert(0, \"Year\", year)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"final_taiwan_stock_dividends_with_year.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4aa44-4bed-4524-89b7-382fa6c72631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad777c10-e66e-4c5d-ae4b-fa8c4b9d6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "# Extract year from filename\n",
    "filename = \"t05st09_2_e_2024.html\"\n",
    "year_match = re.search(r\"t05st09_2_e_(\\d{4})\", filename)\n",
    "year = year_match.group(1) if year_match else \"Unknown\"\n",
    "\n",
    "# Read the HTML file using Big5 encoding\n",
    "with open(filename, \"r\", encoding=\"big5\", errors=\"ignore\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# Wrap HTML content in StringIO to avoid FutureWarning\n",
    "html_io = StringIO(str(soup))\n",
    "\n",
    "# Extract all tables using pandas\n",
    "tables = pd.read_html(html_io)\n",
    "\n",
    "# Extract and clean the third table\n",
    "if len(tables) > 2:\n",
    "    df = tables[2]\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df = df[df[df.columns[0]] != df.columns[0]]\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].fillna(\"Company\").replace(\"\", \"Company\")\n",
    "    df.columns = df.iloc[1]  # Set second row as header\n",
    "    df = df[2:]              # Remove the first two rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add the Year column with correct value\n",
    "    df.insert(0, \"Year\", year)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"final_taiwan_stock_dividends_with_year.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3780afa-9298-4614-adfe-215b0dbebb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70ef87-aed5-49b3-98dc-fe392a1c8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc15b4-fc69-4178-a3c5-7cb63dbb3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Company code'] == '2412']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be819b75-2538-4fad-905c-0aca1a11ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Initialize an empty list to collect DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Loop through years 2015 to 2025\n",
    "for year in range(2015, 2026):\n",
    "    filename = f\"t05st09_2_e_{year}.html\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, \"r\", encoding=\"big5\", errors=\"ignore\") as file:\n",
    "                soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "            html_io = StringIO(str(soup))\n",
    "            tables = pd.read_html(html_io)\n",
    "\n",
    "            if len(tables) > 2:\n",
    "                df = tables[2]\n",
    "                df.dropna(how='all', inplace=True)\n",
    "                df = df[df[df.columns[0]] != df.columns[0]]\n",
    "                df.iloc[:, 0] = df.iloc[:, 0].fillna(\"Company\").replace(\"\", \"Company\")\n",
    "                df.columns = df.iloc[1]  # Set second row as header\n",
    "                df = df[2:]              # Remove the first two rows\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                df.insert(0, \"Year\", str(year))\n",
    "                all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "if all_dataframes:\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    combined_df.to_csv(\"all_years_taiwan_stock_dividends.csv\", index=False)\n",
    "    print(\"Combined CSV file 'all_years_taiwan_stock_dividends.csv' has been created successfully.\")\n",
    "else:\n",
    "    print(\"No valid files found to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f4773-d772-45c2-b01e-dcbde479b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Company code'] == '2412']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a027b0-9a32-4d0b-90be-2e9281891868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe37964-2ed0-44c0-bce3-98c0deae3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[combined_df['Company code'] == '2412']['Cash dividends distributed from retained earnings (NTD/share)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c14247-d2f1-458d-b364-56b72dd2a485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
